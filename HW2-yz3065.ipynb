{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot.T\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = '/Users/zhengyixing/Documents/study/3nd_semester/DL/homework/cifar10-hw1/'\n",
    "X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test').T\n",
    "print('Data loading done')\n",
    "indexs = np.random.choice(50000, 5000, replace=False)\n",
    "X_validation = X_train[:,indexs].T\n",
    "y_validation = y_train[indexs]\n",
    "X_train = np.delete(X_train,indexs, axis = 1).T\n",
    "y_train = np.delete(y_train,indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(X_validation.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        start_index = np.random.randint(0, m - batch_size)\n",
    "        X_batch = X[start_index:(start_index + batch_size), :]\n",
    "        y_batch = y[start_index:(start_index + batch_size)]\n",
    "        \n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "epoch = 50\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.name_scope('input') as scope:\n",
    "        X = tf.placeholder(tf.float32, shape = (None, 3072))\n",
    "        y = tf.placeholder(tf.int32, shape = (None))\n",
    "        dropout_rate = tf.placeholder(tf.float32, shape=())\n",
    "        training = tf.placeholder(tf.bool)\n",
    "        input_layer = tf.reshape(X, [-1, 32, 32, 3])\n",
    "\n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        conv1 = tf.layers.conv2d(inputs=input_layer, filters=32, kernel_size=3, strides=1, \n",
    "                                padding = 'SAME', activation = tf.nn.relu, name='conv1')\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=2, strides=2, padding='SAME', name='pool1')\n",
    "\n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=3, strides=1, \n",
    "                            padding = 'SAME', activation = tf.nn.relu, name='conv2')\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=2, strides=2, padding='SAME', name='pool2')\n",
    "\n",
    "    with tf.name_scope('fc') as scope:\n",
    "        pool2_flat = tf.reshape(pool2, [-1, 8*8*64], name='pool2_flat')\n",
    "        dense = tf.layers.dense(pool2_flat, units = 8*64, activation = tf.nn.relu, name='dense')\n",
    "        dropout = tf.layers.dropout(dense, rate=dropout_rate, training=training, name='dropout')\n",
    "\n",
    "    with tf.name_scope('logits') as scope:\n",
    "        logits = tf.layers.dense(dropout, units = 10, name='logits')\n",
    "\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        softmax = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name = 'softmax')\n",
    "        loss = tf.reduce_mean(softmax, name = 'loss')    \n",
    "\n",
    "    with tf.name_scope('train') as scope:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    with tf.name_scope('eval') as scope:\n",
    "        correct = tf.nn.in_top_k(logits, y, 1, name='correct')\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope('summary') as scope:\n",
    "        loss_summary = tf.summary.scalar('LOSS', loss)\n",
    "        accuracy_symmary = tf.summary.scalar('ACCURACY', accuracy)\n",
    "        test_loss_summary = tf.summary.scalar('test_LOSS', loss)\n",
    "        test_accuracy_symmary = tf.summary.scalar('test_ACCURACY', accuracy)\n",
    "        file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "    with tf.name_scope('init') as scope:\n",
    "        init = tf.global_variables_initializer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_validation, y_validation, batch_size, epoch):\n",
    "    config = tf.ConfigProto()\n",
    "    config.allow_soft_placement = True\n",
    "    \n",
    "    training_num = X_train.shape[0]\n",
    "    iteration = training_num // batch_size\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(epoch):\n",
    "            test_loss_summary_str =  test_loss_summary.eval(feed_dict={X: X_validation, y:y_validation,\n",
    "                                                             dropout_rate:0.3, training:False})\n",
    "            test_accuracy_symmary_str = test_accuracy_symmary.eval(feed_dict={X: X_validation, y:y_validation,\n",
    "                                                             dropout_rate:0.3, training:False})\n",
    "            file_writer.add_summary(test_loss_summary_str, i)\n",
    "            file_writer.add_summary(test_accuracy_symmary_str, i)\n",
    "            \n",
    "            for j in range(iteration):\n",
    "                X_batch, y_batch = get_batch(X_train, y_train, batch_size)\n",
    "                sess.run(optimizer, feed_dict={X: X_batch, y:y_batch, dropout_rate:0.3, training:True})\n",
    "                step = j + i * iteration\n",
    "                if step % 100 == 0:\n",
    "                        loss_ = loss.eval(feed_dict={X: X_batch, y:y_batch, dropout_rate:0.3, training:False})\n",
    "                        accuracy_ = accuracy.eval(feed_dict={X: X_batch, y:y_batch,\n",
    "                                                             dropout_rate:0.3, training:False})\n",
    "                        loss_summary_str =  loss_summary.eval(feed_dict={X: X_batch, y:y_batch,\n",
    "                                                                          dropout_rate:0.3, training:False})\n",
    "                        accuracy_symmary_str = accuracy_symmary.eval(feed_dict={X: X_batch, y:y_batch,\n",
    "                                                             dropout_rate:0.3, training:False})\n",
    "                        print('after '+ str(step) + ' iterations' + \n",
    "                              'the loss is ' + str(loss_) + ', the accuracy is ' + str(accuracy_))\n",
    "                        file_writer.add_summary(loss_summary_str, step)\n",
    "                        file_writer.add_summary(accuracy_symmary_str, step)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 iterationsthe loss is 2.32247, the accuracy is 0.109375\n",
      "after 100 iterationsthe loss is 2.29982, the accuracy is 0.117188\n",
      "after 200 iterationsthe loss is 2.29606, the accuracy is 0.109375\n",
      "after 300 iterationsthe loss is 2.27701, the accuracy is 0.101562\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, X_validation, y_validation, batch_size, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
